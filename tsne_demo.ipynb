{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Original tSNE paper\n",
    "The original tSNE paper explains the algorithm and gives a specific algorithm to optimise a projection to minimise the tSNE loss.\n",
    "[L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008.  PDF](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)\n",
    "\n",
    "## Parametric tSNE paper\n",
    "\n",
    "[L.J.P. van der Maaten. Learning a Parametric Embedding by Preserving Local Structure. In Proceedings of the Twelfth International Conference on Artificial Intelligence & Statistics (AI-STATS), JMLR W&CP 5:384-391, 2009. PDF](https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf)\n",
    "\n",
    "\n",
    "### Original notebook \n",
    "This notebook is derived from Kyle McDonald's notebook. You can find the original code here.\n",
    "https://github.com/kylemcdonald/Parametric-t-SNE/blob/master/Parametric%20t-SNE%20(Keras).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import the keras bits we need\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.constraints import nonneg\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "This notebook briefly shows how tSNE can be implemented as a straightforward loss function, which uses a precomputed \"neighbourly-ness\" distribution and compares a projection against that expected distribution. This is optimised to find a good layout of data in a low-d space.\n",
    "\n",
    "This loss function can be used to find a projection of high-d data to a low-d space that preserves neighbour relations. It can be used on its own, or as a contributing part of an autoencoder. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We work on mini batches of data\n",
    "# Note that we have to form a batch_size x batch_size\n",
    "# distance matrix. If batch_size is too high, we'll run out of memory\n",
    "batch_size = 5000\n",
    "\n",
    "## Load MNIST \n",
    "(x_train,y_train_labels), (x_test,y_test_labels) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype(np.float32)/255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(np.float32)/255.0\n",
    "\n",
    "print x_train.shape[0], 'train samples'\n",
    "print x_test.shape[0], 'test samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot some random samples from the data\n",
    "for i in range(64):\n",
    "    plt.subplot(8,8,i+1)\n",
    "    ix = np.random.randint(0, x_train.shape[0])\n",
    "    plt.imshow(x_train[ix,:].reshape((28,28)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the P matrix\n",
    "These function compute the probability that each pair of points $(x_i,x_j)$ should be \"together\", using a Gaussian kernel distance function.\n",
    "\n",
    "$$p_{ij} = \\frac{\\exp( - ||x_i - x_j||^2 / 2 \\sigma^2)}\n",
    "{\\sum_{k \\neq l} \\exp( - ||x_k - x_l||^2 / 2 \\sigma^2)} \\quad (1)$$\n",
    "\n",
    "This is \"symmetrised\" to create a new variable $p^\\prime_{ij}$ such that $p^\\prime_{ij} = p^\\prime_{ji}$, by setting\n",
    "$p^\\prime_{ij} = \\frac{p_{ij}}{2} + \\frac{p_{ji}}{2}$ **(pp. 2584)**\n",
    "\n",
    "\n",
    "### Searching for $\\sigma$\n",
    "The width of the kernel $\\sigma_i$ *for each datapoint* is adjusted by binary search so that the perplexity of the neighbourhood distribution for each point matches the a specified perplexity (perplexity = 2^entropy). \n",
    "\n",
    "This search adjusts $\\sigma_i$ to make the distribution of $p_i$ have a fixed entropy for all $i$. The tSNE paper presents this as a kind of smooth \"nearest neighbour count\" -- each point $i$ should be influenced by a similar number of neighbours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Hbeta(D, beta):\n",
    "    P = np.exp(-D * beta)\n",
    "    sumP = np.sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "def x2p(X, u=15, tol=1e-4, print_iter=500, max_tries=50, verbose=0):\n",
    "    # Initialize some variables\n",
    "    n = X.shape[0]                     # number of instances\n",
    "    P = np.zeros((n, n))               # empty probability matrix\n",
    "    beta = np.ones(n)                  # empty precision vector\n",
    "    logU = np.log(u)                   # log of perplexity (= entropy)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    if verbose > 0: print('Computing pairwise distances...')\n",
    "    sum_X = np.sum(np.square(X), axis=1)\n",
    "    # note: translating sum_X' from matlab to numpy means using reshape to add a dimension\n",
    "    D = sum_X + sum_X[:,None] + -2 * X.dot(X.T)\n",
    "\n",
    "    # Run over all datapoints\n",
    "    if verbose > 0: print('Computing P-values...')\n",
    "    for i in range(n):\n",
    "        \n",
    "        if verbose > 1 and print_iter and i % print_iter == 0:\n",
    "            print('Computed P-values {} of {} datapoints...'.format(i, n))\n",
    "        \n",
    "        # Set minimum and maximum values for precision\n",
    "        betamin = float('-inf')\n",
    "        betamax = float('+inf')\n",
    "        \n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        indices = np.concatenate((np.arange(0, i), np.arange(i + 1, n)))\n",
    "        Di = D[i, indices]\n",
    "        H, thisP = Hbeta(Di, beta[i])\n",
    "        \n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while abs(Hdiff) > tol and tries < max_tries:\n",
    "            \n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i]\n",
    "                if np.isinf(betamax):\n",
    "                    beta[i] *= 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2\n",
    "            else:\n",
    "                betamax = beta[i]\n",
    "                if np.isinf(betamin):\n",
    "                    beta[i] /= 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2\n",
    "            \n",
    "            # Recompute the values\n",
    "            H, thisP = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "        \n",
    "        # Set the final row of P\n",
    "        P[i, indices] = thisP\n",
    "        \n",
    "    if verbose > 0: \n",
    "        print('Mean value of sigma: {}'.format(np.mean(np.sqrt(1 / beta))))\n",
    "        print('Minimum value of sigma: {}'.format(np.min(np.sqrt(1 / beta))))\n",
    "        print('Maximum value of sigma: {}'.format(np.max(np.sqrt(1 / beta))))\n",
    "    \n",
    "    return P, beta\n",
    "\n",
    "def compute_joint_probabilities(samples, batch_size=5000, d=2, perplexity=30, tol=1e-5, verbose=0):\n",
    "    v = d - 1\n",
    "    \n",
    "    # Initialize some variables\n",
    "    n = samples.shape[0]\n",
    "    batch_size = min(batch_size, n)\n",
    "    \n",
    "    # Precompute joint probabilities for all batches\n",
    "    if verbose > 0: print('Precomputing P-values...')\n",
    "    batch_count = int(n / batch_size)\n",
    "    P = np.zeros((batch_count, batch_size, batch_size))\n",
    "    for i, start in enumerate(range(0, n - batch_size + 1, batch_size)):   \n",
    "        curX = samples[start:start+batch_size]                   # select batch\n",
    "        P[i], beta = x2p(curX, perplexity, tol, verbose=verbose) # compute affinities using fixed perplexity\n",
    "        P[i][np.isnan(P[i])] = 0                                 # make sure we don't have NaN's\n",
    "        P[i] = (P[i] + P[i].T) # / 2                             # make symmetric\n",
    "        P[i] = P[i] / P[i].sum()                                 # obtain estimation of joint probabilities\n",
    "        P[i] = np.maximum(P[i], np.finfo(P[i].dtype).eps)\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting P\n",
    "Now we actually run the code and compute the P matrix for our data. The output is an `n x batch_size x batch_size` matrix, where `n` is the number of batches. \n",
    "\n",
    "I shuffled the data to make sure each minibatch has similar distribution.\n",
    "\n",
    "### Perplexity\n",
    "The perplexity is the key parameter that controls the tSNE layout. Values between 15 and 80 are usually suitable. The paper claims that tSNE is mostly invariant to the setting of this parameter, but I've found that is relatively important (but fairly smooth: 13 will look like 14).\n",
    "\n",
    "Perplexity is:\n",
    "$$Perp(P_i) = 2^{H(P_i}),$$ and the $H(P_i)$ is the Shannon entropy:\n",
    "$$H(P_i)=-\\sum_j p_{ji} log_2(p_{ji}).$$\n",
    "\n",
    "So we optimise $\\sigma_i$ to give a similar $H(P_i)$ for each point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute P matrix\n",
    "# this can take a little while\n",
    "np.random.shuffle(x_train)\n",
    "P = compute_joint_probabilities(x_train, batch_size=5000, perplexity=60, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the P matrix\n",
    "We can see what the P matrix looks like. We just plot a subsection of the first minibatch P matrix, because the whole set of P matrices is huge. The result won't be very interesting, because the data order is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "# show the log matrix\n",
    "P0 = np.array(P[0][:500, :500])\n",
    "plt.imshow(np.log(P0), interpolation=\"nearest\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tSNE loss\n",
    "To optimise our projection, we use the tSNE loss as an objective function. \n",
    "\n",
    "This loss is given by the Kullback-Leibler divergence between the high-dimensional neighbourhood distribution (P) and the a low-dimensional neighbourhood distribution (Q). The Q distribution uses a t-distributed neighbourhood function instead of a Gaussian to account for the distortion of space in down-projecting (hence **t**SNE).\n",
    "\n",
    "$$ q_{ij} = \\frac{(1+||y_i - y_j||^2)^{-1}}{\\sum_{k\\neq l}(1+||y_k + y_l||^2)^{-1}} \\quad (4)$$ \n",
    "\n",
    "$$C=KL(P||Q)=\\sum_i\\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$\n",
    "\n",
    "This is differentiable, and Theano works out the derivatives for us.\n",
    "\n",
    "`tsne(P,x)` takes the precomputed P matrix and a set of projected values (for one minibatch) and returns the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The t-distribution\n",
    "The use of the t-distribution seems to be very important in getting good results in the projection. The heavy tails allow the low-dimensional neighbourhood to \"stretch\" much further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show the difference between the T and Gaussian distributions\n",
    "x = np.linspace(-5,5, 200)\n",
    "gauss = np.exp(-x**2)\n",
    "t =  (1+x**2)**(-1)\n",
    "plt.plot(x,gauss, lw=2)\n",
    "plt.plot(x,t, '--', lw=2)\n",
    "plt.fill_between(x, gauss, t, hatch='//', facecolor='none', alpha=0.2)\n",
    "plt.legend([\"Gaussian\", \"t distribution $\\\\nu=1$\"])\n",
    "plt.figure()\n",
    "\n",
    "plt.semilogy(x,gauss, lw=2)\n",
    "plt.semilogy(x,t, '--', lw=2)\n",
    "plt.fill_between(x, gauss, t, hatch='//', facecolor='none', alpha=0.2)\n",
    "plt.legend([\"Gaussian\", \"t distribution $\\\\nu=1$\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# P is the joint probabilities for this batch (Keras loss functions call this y_true)\n",
    "# activations is the low-dimensional output (Keras loss functions call this y_pred)\n",
    "\n",
    "def tsne(P, activations):\n",
    "#     d = K.shape(activations)[1]\n",
    "    d = 2 # TODO: should set this automatically, but the above is very slow for some reason\n",
    "    n = 5000 #batch_size # TODO: should set this automatically\n",
    "    v = d - 1.\n",
    "    eps = K.variable(10e-15) # needs to be at least 10e-8 to get anything after Q /= K.sum(Q)\n",
    "    sum_act = K.sum(K.square(activations), axis=1)\n",
    "    Q = K.reshape(sum_act, [-1, 1]) + -2 * K.dot(activations, K.transpose(activations))\n",
    "    Q = (sum_act + Q) / v\n",
    "    Q = K.pow(1 + Q, -(v + 1) / 2)\n",
    "    Q *= K.variable(1 - np.eye(n))\n",
    "    Q /= K.sum(Q)\n",
    "    Q = K.maximum(Q, eps)\n",
    "    C = K.log((P + eps) / (Q + eps))\n",
    "    C = K.sum(P * C)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight tSNE\n",
    "We can directly optimise the tSNE using a MLP structure. We map inputs to a two-dimensional space, and then backprop to update weights to minimise the tSNE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple one-hidden-layer MLP\n",
    "# 784 - 256 - 2 (tsne) \n",
    "\n",
    "input_img = Input(shape=(x_train.shape[1],))\n",
    "l = input_img\n",
    "# hidden layer\n",
    "l = Dense(256,  activation='relu')(l)\n",
    "# output layer\n",
    "l = Dense(2, activation='linear')(l)\n",
    "\n",
    "# note that tSNE gets the P matrix from the targets. \n",
    "tsne_model = Model(input=input_img, output=l)\n",
    "sgd = SGD(lr=0.1)\n",
    "# Use stochastic gradient descent\n",
    "tsne_model.compile(loss=tsne, optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "The only subtlety is that the target is the P matrix -- that is what the tsne loss compares the outputs to, via the Q matrix which it forms internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outputs are from the P matrix\n",
    "y_train = P.reshape(x_train.shape[0], -1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "# must not shuffle or we might mix rows from different P matrices!\n",
    "tsne_model.fit(x_train, y_train, batch_size=5000, shuffle=False, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "The output can now be found via `.predict()` Note that because we have learned a **parametric model**, we can project new, unseen data onto this space if we want to. We haven't just learned where the new points go, but the transformation that takes them there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "embedding = tsne_model.predict(x_train)\n",
    "plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=y_train_labels, cmap=\"viridis\")\n",
    "plt.figure()\n",
    "embedding = tsne_model.predict(x_test)\n",
    "plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=y_test_labels, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tSNE autoencoder\n",
    "We can use the tSNE loss as a contributing loss function in any model. We just need to have a layer with the right dimension which we can compute a Q matrix from.\n",
    "### Autoencoder\n",
    "We can show this by building an autoencoder, and optimising the loss on both the reconstruction error (how well can the autoencoder reconstruct the input after going through the latent variable bottleneck), **and** the tSNE loss (how similar is the neighbourhood distribution of the high dimensional space to the neighoburhood distribution of the latent variable bottleneck)\n",
    "\n",
    "### Inversion\n",
    "Note that we could use to **invert** the transform, since the layers from the latent space to the reconstructed space reconstruct a high-d point given a low-d point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.constraints import nonneg\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "# Deep autoencoder, with the latent layer subject to the tSNE penalty\n",
    "# 784 - 256 - 128 - 16 - 2 (tsne) - 16 - 128 - 256 - 784\n",
    "\n",
    "input_img = Input(shape=(x_train.shape[1],))\n",
    "l = input_img\n",
    "l = Dense(256,  activation='relu')(l)\n",
    "l = Dense(128,  activation='relu')(l)\n",
    "l = Dense(16, activation='relu')(l)\n",
    "# this is the latent variable bottleneck layer (here, with D=2)\n",
    "l = Dense(2)(l)\n",
    "\n",
    "# note that we keep a hold of the projected layer; we apply the tSNE loss\n",
    "# to this output layer\n",
    "proj_layer = l\n",
    "\n",
    "l2=Dense(16, activation='relu')(l)\n",
    "l2=Dense(128,  activation='relu')(l2)\n",
    "l2=Dense(256,  activation='relu')(l2)\n",
    "l2=Dense(784, activation='linear')(l2)\n",
    "# the last layer should be the reconstructed values\n",
    "\n",
    "out_layer = l2\n",
    "\n",
    "# we specify *two* outputs for this model\n",
    "auto_model = Model(input=input_img, output=[out_layer, proj_layer])\n",
    "\n",
    "# we also create the inverse model\n",
    "latent_input = Input(shape=(2,))\n",
    "decoder_layers = auto_model.layers[-4:]\n",
    "\n",
    "# surely this can't be the right way of doing this?!\n",
    "decoder = reduce(lambda x,y:y(x), decoder_layers, latent_input)\n",
    "inverse_model = Model(input=latent_input, output=decoder)\n",
    "\n",
    "sgd = SGD(lr=0.1)\n",
    "# and specify dual loss functions: mse + tsne\n",
    "auto_model.compile(loss=['mean_squared_error',tsne], optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# note the two outputs: first one is autoencoder reconstruction, second is the P matrix\n",
    "auto_model.fit(x_train, [x_train, y_train], batch_size=5000, shuffle=False, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# use the second output -- the latent variables\n",
    "embedding = auto_model.predict(x_train)[1]\n",
    "plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=y_train_labels, cmap=\"viridis\")\n",
    "plt.figure()\n",
    "embedding = auto_model.predict(x_test)[1]\n",
    "plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=y_test_labels, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the inversion\n",
    "We can try randomly generating a point near an existing point and see what we get as a reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# use the second output -- the latent variables\n",
    "embedding = auto_model.predict(x_train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# choose a random point somewhere on a line between two existing points\n",
    "def random_inverse_digit(embedding, inverse_model):\n",
    "    n1, n2 = random.choice(embedding), random.choice(embedding)\n",
    "    t = np.random.uniform(0,1)\n",
    "    n = t*n1 + (1-t)*n2\n",
    "    digit = inverse_model.predict([n[None,:]])\n",
    "    print digit.shape\n",
    "    plt.imshow(digit.reshape(28,28), cmap=\"gray\", interpolation=\"nearest\")\n",
    "    \n",
    "random_inverse_digit(embedding, inverse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deep autoencoder\n",
    "We can make any kind of autoencoder structure, and keep the tSNE loss. This model is a deep denoising autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.constraints import nonneg\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.models import Model\n",
    "\n",
    "# Deep autoencoder, with the latent layer subject to the tSNE penalty\n",
    "# 784 - 256 - 128 - 16 - 2 (tsne) - 16 - 128 - 256 - 784\n",
    "\n",
    "# std. dev. of noise added during training\n",
    "sigma = 0.25\n",
    "\n",
    "input_img = Input(shape=(x_train.shape[1],))\n",
    "l = input_img\n",
    "l = GaussianNoise(sigma)(l)\n",
    "l = Dense(256,  activation='relu')(l)\n",
    "l = Dense(128,  activation='relu')(l)\n",
    "l = Dense(16, activation='relu')(l)\n",
    "# this is the latent variable bottleneck layer (here, with D=2)\n",
    "l = Dense(2)(l)\n",
    "\n",
    "# note that we keep a hold of the projected layer; we apply the tSNE loss\n",
    "# to this output layer\n",
    "proj_layer = l\n",
    "\n",
    "l=Dense(16, activation='relu')(l)\n",
    "l=Dense(128,  activation='relu')(l)\n",
    "l=Dense(256,  activation='relu')(l)\n",
    "l=Dense(784, activation='linear')(l)\n",
    "# the last layer should be the reconstructed values\n",
    "\n",
    "out_layer = l\n",
    "\n",
    "# we specify *two* outputs for this model\n",
    "auto_model = Model(input=input_img, output=[out_layer, proj_layer])\n",
    "\n",
    "# we also create the inverse model\n",
    "latent_input = Input(shape=(2,))\n",
    "# surely this can't be the right way of doing this?!\n",
    "decoder = reduce(lambda x,y:y(x), decoder_layers, latent_input)\n",
    "inverse_model = Model(input=latent_input, output=decoder)\n",
    "\n",
    "sgd = SGD(lr=0.1)\n",
    "# and specify dual loss functions: mse + tsne\n",
    "auto_model.compile(loss=['mean_squared_error',tsne], optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test the deep denoising autoencoder\n",
    "plt.figure()\n",
    "# use the second output -- the latent variables\n",
    "embedding = auto_model.predict(x_train)[1]\n",
    "plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=y_train_labels, cmap=\"viridis\")\n",
    "plt.figure()\n",
    "embedding = auto_model.predict(x_test)[1]\n",
    "plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=y_test_labels, cmap=\"viridis\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "# test inversion\n",
    "random_inverse_digit(embedding, inverse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
